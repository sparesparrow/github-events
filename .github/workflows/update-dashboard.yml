name: ğŸ“Š Update GitHub Events Dashboard

on:
  schedule:
    # Update dashboard every 15 minutes
    - cron: '*/15 * * * *'
  workflow_dispatch:
    # Manual trigger for testing
  push:
    branches: [ main ]
    paths: 
      - 'src/**'
      - '.github/workflows/update-dashboard.yml'

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  update-dashboard:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ”„ Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: ğŸ—„ï¸ Initialize database
      run: |
        mkdir -p database
        python -c "
        import asyncio
        from src.github_events_monitor.collector import GitHubEventsCollector
        
        async def init():
            collector = GitHubEventsCollector('database/events.db', '${{ secrets.GITHUB_TOKEN }}')
            await collector.initialize_database()
            print('Database initialized')
        
        asyncio.run(init())
        "

    - name: ğŸ“¥ Collect GitHub Events
      run: |
        python -c "
        import asyncio
        from src.github_events_monitor.collector import GitHubEventsCollector
        
        async def collect():
            collector = GitHubEventsCollector('database/events.db', '${{ secrets.GITHUB_TOKEN }}')
            await collector.initialize_database()
            count = await collector.collect_and_store(limit=50)
            print(f'Collected {count} events')
        
        asyncio.run(collect())
        "

    - name: ğŸ“Š Export dashboard data
      run: |
        python -c "
        import asyncio
        import json
        from datetime import datetime, timezone
        from src.github_events_monitor.collector import GitHubEventsCollector
        
        async def export():
            collector = GitHubEventsCollector('database/events.db', '${{ secrets.GITHUB_TOKEN }}')
            
            # Get metrics
            counts_10 = await collector.get_event_counts_by_type(10)
            counts_60 = await collector.get_event_counts_by_type(60)
            trending = await collector.get_trending_repositories(24, 20)
            
            # Generate dashboard data
            events_by_type = []
            today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
            
            for event_type, count in counts_60.items():
                if count > 0:
                    events_by_type.append({
                        'type': event_type,
                        'day': today,
                        'count': count
                    })
            
            top_repositories = []
            for repo in trending:
                top_repositories.append({
                    'repo_name': repo['repo_name'],
                    'total_events': repo['total_events'],
                    'watches': repo['watch_events'],
                    'pull_requests': repo['pr_events'],
                    'issues': repo['issue_events']
                })
            
            dashboard_data = {
                'events_by_type_date': events_by_type,
                'top_repositories': top_repositories,
                'last_updated': datetime.now(timezone.utc).isoformat()
            }
            
            # Export all data files
            with open('docs/data.json', 'w') as f:
                json.dump(dashboard_data, f, indent=2)
            
            with open('docs/trending.json', 'w') as f:
                json.dump({'status': 200, 'data': {'hours': 24, 'repositories': trending}}, f, indent=2)
            
            with open('docs/event_counts_10.json', 'w') as f:
                json.dump(counts_10, f, indent=2)
            
            with open('docs/event_counts_60.json', 'w') as f:
                json.dump(counts_60, f, indent=2)
            
            with open('docs/data_status.json', 'w') as f:
                json.dump({
                    'base_url': 'https://sparesparrow.github.io/github-events',
                    'health_status': 200,
                    'event_counts_10_status': 200,
                    'event_counts_60_status': 200,
                    'trending_status': 200,
                    'generated_at': datetime.now(timezone.utc).isoformat()
                }, f, indent=2)
            
            print(f'âœ… Dashboard data exported - {sum(counts_60.values())} events in 60min, {len(trending)} trending repos')
        
        asyncio.run(export())
        "

    - name: ğŸ“ˆ Generate visualization charts
      run: |
        python -c "
        import requests
        import json
        from datetime import datetime
        
        # Generate trending chart via API if available
        try:
            # This would normally call our API, but for CI we'll skip the chart generation
            # as it requires the full API server to be running
            print('Chart generation skipped in CI environment')
        except Exception as e:
            print(f'Chart generation error (expected in CI): {e}')
        "

    - name: ğŸ“ Commit updated data
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        if git diff --quiet docs/; then
          echo "No changes to commit"
        else
          git add docs/
          git commit -m "ğŸ“Š Auto-update dashboard data - $(date -u +'%Y-%m-%d %H:%M UTC')
          
          ğŸ“ˆ Latest GitHub Events data refreshed
          ğŸ”„ Automated by GitHub Actions
          ğŸ•’ Generated: $(date -u --iso-8601=seconds)"
          
          git push
          echo "âœ… Dashboard data updated and pushed"
        fi

    - name: ğŸš€ Deploy to GitHub Pages
      uses: actions/deploy-pages@v3
      with:
        path: docs/
